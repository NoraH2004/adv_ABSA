{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Word Detection\n",
    "\n",
    "1. Load Dataset SemEval 2015 laptops ABSA\n",
    "2. Do sentence splitting\n",
    "3. Leave One Out (LOO)\n",
    "\n",
    "4. Predict Sentences\n",
    "5. Find important word by comparing the original sentences with the modified ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import utils.dataloader as dl\n",
    "import utils.text_processing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SemEval_2015_laptops/absa_15_laptops_train_data.xml'\n",
    "\n",
    "#test data: contains only 10 entries, no duplicates:\n",
    "#filename = 'SemEval_2015_laptops/test_data.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1971"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from xml file into required format\n",
    "sentences, aspect_category_sentiments, (idx2aspectlabel, idx2sentilabel), cats = dl.semeval_to_aspectsentiment_hr(filename)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate sentences\n",
    "sentences = list(dict.fromkeys(sentences))\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists of tokenized sentences\n",
    "tok_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tok_sentences.append(sentence.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Leave One Out (LOO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396 1971\n"
     ]
    }
   ],
   "source": [
    "# go over the list of tokens in a sentence\n",
    "# and drop each word after the other\n",
    "# go over sentences in list of tokenized sentences\n",
    "\n",
    "sentence_packages = []\n",
    "for sent in range(len(tok_sentences)):\n",
    "    original_sentence = tp.detokenize(tok_sentences[sent])\n",
    "    \n",
    "# go over token in sentence    \n",
    "    modified_sentences = []\n",
    "    for token in range(len(tok_sentences[sent])):\n",
    "        tok_mod_sentence = tp.get_token_dropped_sentence_at_pos(tok_sentences[sent], token)\n",
    "        modified_sentences.append((tok_sentences[sent][token], tp.detokenize(tok_mod_sentence)))\n",
    "    sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )\n",
    "print(len(sentence_packages),len(aspect_category_sentiments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ABSA BERT and all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = open(\"security/key.txt\", \"r\")\n",
    "key = k.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Predictor\n",
      "Loading model models/en-laptops-absa\n",
      "Config loaded from models/en-laptops-absa/config.json\n",
      "Aspects loaded from models/en-laptops-absa/aspects.jsonl\n",
      "Config loaded from models/en-laptops-absa/config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from absa import Predictor\n",
    "from security import Authorization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pred = Predictor(os.path.join('models','en-laptops-absa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference to a free GPU\n",
    "!export CUDA_VISIBLE_DEVICE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict Sentences\n",
    "#### Prediction of the original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for package in sentence_packages:\n",
    "    document = {'text': package['original_sentence'], 'segments':[{'span':[0,0],'text': package['original_sentence']}]}\n",
    "    documents.append(document)\n",
    "    \n",
    "results = pred.predict(documents, key, with_segments=True)\n",
    "\n",
    "original_results = []\n",
    "for result in results:\n",
    "    original_results.append(result[0])\n",
    "#print('original_results: ', original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of modified sentences\n",
    "##### We have to create a Indexlist in order to avoid predicting sentence_sets but rather predict all sentences at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19605\n",
      "1396\n"
     ]
    }
   ],
   "source": [
    "# create list with indices where every sentence from one package has the same index\n",
    "# e.g. indices = [1,1,1,1,2,2,2,2,3,3,4,4,4,4,4,4,4]\n",
    "# this would be the indices for 4 sentences\n",
    "\n",
    "# this is done to predict all modified sentences in one run \n",
    "# and be able to map them together accordingly afterwards.\n",
    "\n",
    "package_indices = []\n",
    "package_index = 0\n",
    "for package in sentence_packages:\n",
    "    package_index += 1\n",
    "    for mod_sent in package['modified_sentences']:\n",
    "        package_indices.append(package_index)\n",
    "        \n",
    "print(len(package_indices))\n",
    "print(package_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_results_unmapped:  19605\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "documents = []\n",
    "\n",
    "package_indices = []\n",
    "package_index = 0\n",
    "\n",
    "for package in sentence_packages:\n",
    "    package_index += 1\n",
    "    for mod_set in package['modified_sentences']:  \n",
    "        package_indices.append(package_index)\n",
    "        documents.append({'text': mod_set[1], 'segments':[{'span':[0,0],'text': mod_set[1]}]})\n",
    "    \n",
    "results = pred.predict(documents, key, with_segments=True)\n",
    "\n",
    "modified_results_unmapped = []\n",
    "for result in results:\n",
    "    modified_results_unmapped.append(result[0])\n",
    "print('modified_results_unmapped: ', len(modified_results_unmapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n",
      "[[{'text': 'computer is absolutely AMAZING!!!', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}, {'text': 'This is absolutely AMAZING!!!', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}, {'text': 'This computer absolutely AMAZING!!!', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}, {'text': 'This computer is AMAZING!!!', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}, {'text': 'This computer is absolutely', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}]]\n"
     ]
    }
   ],
   "source": [
    "# this is the mapping of the modified results, as described before\n",
    "\n",
    "modified_results = []\n",
    "modified_sentence = []\n",
    "check = 1\n",
    "for e, result in enumerate(modified_results_unmapped):\n",
    "    i = package_indices[e]    \n",
    "    if i == check:\n",
    "        modified_sentence.append(result)\n",
    "    else:\n",
    "        modified_results.append(modified_sentence)\n",
    "        modified_sentence = []\n",
    "        modified_sentence.append(result)\n",
    "    check = i\n",
    "modified_results.append(modified_sentence)\n",
    "print(len(modified_results))\n",
    "print(modified_results[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare modified with original\n",
    "# if they are the same, drop the modified sentence\n",
    "# if something is different, append the modified sentence to the database ready to be attacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparison of Results of Leave One Out Method\n",
    "\n",
    "##### compare modified with original\n",
    "##### if they are the same, drop the modified sentence\n",
    "##### if something is different, append the modified sentence to the database ready to be attacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a new important words list and a new sentence package list\n",
    "#### Onyl those packages, where there was a modification possible through the LOO method should be worked on\n",
    "\n",
    "##### only those modified sentences, where there was a change possible should be in the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694\n",
      "943\n",
      "[{'original_sentence': 'super fast processor and really nice graphics card..', 'original_result': [{'aspect': 'Graphics', 'sentiment': 'POS'}, {'aspect': 'CPU', 'sentiment': 'POS'}, {'aspect': 'Performance', 'sentiment': 'POS'}], 'modified_sentences': [['super processor and really nice graphics card..', 'super fast and really nice graphics card..']], 'modified_results': [[[{'aspect': 'Graphics', 'sentiment': 'POS'}, {'aspect': 'CPU', 'sentiment': 'POS'}], [{'aspect': 'Graphics', 'sentiment': 'POS'}, {'aspect': 'Performance', 'sentiment': 'POS'}]]]}, {'original_sentence': 'and plenty of storage with 250 gb(though I will upgrade this and the ram..)', 'original_result': [{'aspect': 'Storage', 'sentiment': 'POS'}], 'modified_sentences': [['and of storage with 250 gb(though I will upgrade this and the ram..)', 'and plenty of with 250 gb(though I will upgrade this and the ram..)', 'and plenty of storage with gb(though I will upgrade this and the ram..)']], 'modified_results': [[[{'aspect': 'Storage', 'sentiment': 'NEU'}], [{'aspect': 'Memory', 'sentiment': 'POS'}, {'aspect': 'Storage', 'sentiment': 'POS'}], [{'aspect': 'Memory', 'sentiment': 'POS'}, {'aspect': 'Storage', 'sentiment': 'POS'}]]]}]\n"
     ]
    }
   ],
   "source": [
    "# loo_results is a list that displays the modification\n",
    "loo_results = []\n",
    "succesfull_modification_cnt = 0   \n",
    "\n",
    "for e, modified_result_set in enumerate(modified_results):\n",
    "    successfull_modifications_set_txt = []\n",
    "    successfull_modifications_set_aspsent = []\n",
    "    original_result = original_results[e]\n",
    "    o_aspect_sentiment = original_result['aspect_sentiments']\n",
    "    \n",
    "    successfull_modifications_txt = []\n",
    "    successfull_modifications_aspsent = []\n",
    "    for modified_result in modified_result_set:\n",
    "        m_aspect_sentiment = modified_result['aspect_sentiments']\n",
    "        \n",
    "        if o_aspect_sentiment != m_aspect_sentiment:\n",
    "            succesfull_modification_cnt += 1\n",
    "            # print('original_result :', original_result, 'modified_result: ', modified_result)\n",
    "            successfull_modifications_txt.append(modified_result['text'])\n",
    "            successfull_modifications_aspsent.append(modified_result['aspect_sentiments'])\n",
    "    successfull_modifications_set_txt.append(successfull_modifications_txt)\n",
    "    successfull_modifications_set_aspsent.append(successfull_modifications_aspsent)\n",
    "    \n",
    "    if successfull_modifications_txt:\n",
    "        loo_results.append(\n",
    "            {\n",
    "                'original_sentence': original_result['text'],\n",
    "                'original_result': original_result['aspect_sentiments'],\n",
    "                'modified_sentences': successfull_modifications_set_txt,\n",
    "                'modified_results': successfull_modifications_set_aspsent\n",
    "            })\n",
    "        \n",
    "print(succesfull_modification_cnt)   \n",
    "\n",
    "print(len(loo_results))\n",
    "print(loo_results[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Sentence packages with the missing token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_packages = []\n",
    "for item in loo_results:\n",
    "    o_sent = item['original_sentence'].split(' ')\n",
    "    \n",
    "    for lst in item['modified_sentences']:\n",
    "        modified_sentences = []\n",
    "        for sentence in lst:\n",
    "            m_sent = sentence.split()\n",
    "            for word in set(o_sent).difference(set(m_sent)):\n",
    "                word = word\n",
    "            \n",
    "                modified_sentences.append((word, tp.detokenize(m_sent)))\n",
    "    sentence_packages.append(\n",
    "    {\n",
    "        'original_sentence': tp.detokenize(o_sent),\n",
    "        'modified_sentences': modified_sentences\n",
    "    })\n",
    "    \n",
    "#print(sentence_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new important words list:\n",
    "#### a list of list where every list item contains the important words of one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n"
     ]
    }
   ],
   "source": [
    "important_words_packages = []\n",
    "for package in sentence_packages:\n",
    "    important_words = []\n",
    "    for sentence in package['modified_sentences']:\n",
    "        important_words.append(sentence[0])\n",
    "    important_words_packages.append(important_words)\n",
    "\n",
    "print(len(important_words_packages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'important_words_packages' (list)\n",
      "Stored 'sentence_packages' (list)\n",
      "Stored 'loo_results' (list)\n",
      "Stored 'important_words_packages_dev' (list)\n",
      "Stored 'sentence_packages_dev' (list)\n",
      "Stored 'loo_results_dev' (list)\n"
     ]
    }
   ],
   "source": [
    "%store important_words_packages\n",
    "%store sentence_packages\n",
    "%store loo_results\n",
    "\n",
    "# for dev\n",
    "important_words_packages_dev = important_words_packages[:10]\n",
    "sentence_packages_dev = sentence_packages[:10]\n",
    "loo_results_dev = loo_results[:10]\n",
    "\n",
    "%store important_words_packages_dev\n",
    "%store sentence_packages_dev\n",
    "%store loo_results_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
